{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fluid-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from pathlib import Path\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "advanced-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "478c7d4a-2a3a-4c29-9809-1f246c32a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "approved-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "familiar-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = []\n",
    "for t in texts:\n",
    "    if type(t) != str:\n",
    "        continue\n",
    "    tt.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9895957d-91e3-42e2-b619-a22feeeb586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d70f92-dc7f-4fd1-aa5a-88f5e2c6c590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdba9bee-9e80-4709-be66-a0d331dd8652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfbf3623f6f45729b3a293cfc9a041b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21daf7422bb14ed1ac6db576f3b5515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9653fe42818842b4bc992d85a6e1f882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecdf299cff84be681b3d1910a35fa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "513918c0-ec32-4985-8634-6072f1e98723",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(texts, 2000, new_special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5245b6d-3b2c-4b1d-9785-52b0b154303d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " 'in',\n",
       " 'ap',\n",
       " 'er',\n",
       " 'fect',\n",
       " 'wor',\n",
       " 'ld',\n",
       " 'Ġthe',\n",
       " 'Ġ_',\n",
       " 'ir',\n",
       " 'ane',\n",
       " 'le',\n",
       " 'ction',\n",
       " 'Ġwould',\n",
       " 'Ġbe',\n",
       " 'Ġre',\n",
       " 'sol',\n",
       " 'ved',\n",
       " 'Ġand',\n",
       " 'Ġeveryone',\n",
       " 'Ġwould',\n",
       " 'Ġget',\n",
       " 'Ġsome',\n",
       " 'Ġ_',\n",
       " 'good',\n",
       " 'p',\n",
       " 'us',\n",
       " 's',\n",
       " 'y',\n",
       " '.',\n",
       " 'Ġbut',\n",
       " 'Ġwe',\n",
       " 'Ġwon',\n",
       " \"'t\",\n",
       " 'Ġbecause',\n",
       " 'Ġ_',\n",
       " 'go',\n",
       " 'ke',\n",
       " 'y',\n",
       " 'is',\n",
       " 'ad',\n",
       " 'ou',\n",
       " 'che']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"_inaperfectworld the _iranelection would be resolved and everyone would get some _goodpussy. but we won't because _gokeyisadouche\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ebc44d9-98df-4610-a0b4-2200e7a3b42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_2k\\\\tokenizer_config.json',\n",
       " 'gpt2_2k\\\\special_tokens_map.json',\n",
       " 'gpt2_2k\\\\vocab.json',\n",
       " 'gpt2_2k\\\\merges.txt',\n",
       " 'gpt2_2k\\\\added_tokens.json',\n",
       " 'gpt2_2k\\\\tokenizer.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"gpt2_2k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "014ac251-eb06-44f9-8612-f2dba15783e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2_2k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1844d4c-7ff6-487f-86b2-887597654d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0ef2f-b338-4a6c-8697-460d894bf1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c959293-2ead-4334-9328-2447826618c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nonprofit-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "solid-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(vocab_size=2000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "detailed-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "higher-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('bpe_2k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "typical-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer.encode('at jells park x-country relays yesterday go dream team! _team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f42e49-ca1b-423c-a029-d4c5ce4f8a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at ',\n",
       " 'j',\n",
       " 'ell',\n",
       " 's ',\n",
       " 'par',\n",
       " 'k',\n",
       " ' x',\n",
       " '-',\n",
       " 'coun',\n",
       " 't',\n",
       " 'ry ',\n",
       " 're',\n",
       " 'la',\n",
       " 'ys ',\n",
       " 'yester',\n",
       " 'day ',\n",
       " 'go ',\n",
       " 'dream',\n",
       " ' tea',\n",
       " 'm',\n",
       " '! ',\n",
       " '_',\n",
       " 't',\n",
       " 'ea',\n",
       " 'm']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0cb4363-a0d5-4b7f-9714-343f8641205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MSK]\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71d4c38d-b20c-4f7f-a131-d7675f0aafd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[88, 386, 151, 343]], 'token_type_ids': [[0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.batch_encode_plus(['orange.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8e31c2f-03ee-4895-874e-1371c927531f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bpe_2k\\\\tokenizer_config.json',\n",
       " 'bpe_2k\\\\special_tokens_map.json',\n",
       " 'bpe_2k\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(Path('./bpe_2k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f732ca5-3c93-4497-82fd-869176588faf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 3993 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16760\\3039456125.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwrapped_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./bpe_2k/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1783\u001b[1;33m         return cls._from_pretrained(\n\u001b[0m\u001b[0;32m   1784\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1785\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1926\u001b[0m         \u001b[1;31m# Instantiate tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1928\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1929\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfast_tokenizer_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfrom_slow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mfast_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 3993 column 3"
     ]
    }
   ],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast.from_pretrained(Path('./bpe_2k/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f10fa-15a0-4600-a3f4-cc286fd6cb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
