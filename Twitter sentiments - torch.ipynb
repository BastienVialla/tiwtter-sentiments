{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d4ffbe-66ff-4b8e-9a68-7951fae95d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import unidecode\n",
    "import yaml\n",
    "from typing import List\n",
    "\n",
    "from src.architectures.Classifier import *\n",
    "from src.datamodules.LSTMDataModule import *\n",
    "from src.datamodules.LSTMDataModule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6494d9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load & configure tokeninzer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('./tokenizers/gpt2_2k')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0069e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file for the model\n",
    "with open(f'./configs/lstm.yaml', 'r') as in_file:\n",
    "        cfg = yaml.load(in_file, Loader=yaml.FullLoader)\n",
    "cfg['model']['architecture']['pad_id'] = tokenizer.vocab['[PAD]']\n",
    "cfg['model']['architecture']['mask_id'] = tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd3c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean & normlize text\n",
    "def clean(text):\n",
    "    t = text.lower()\n",
    "    t = t.replace('\\\\n', ' ').replace('\\\\t', ' ').replace('\\t', ' ').replace('. com', '.com')\n",
    "    t = re.sub(r'https?:\\/\\/[a-z.\\/A-Z\\d]*', ' ', t)\n",
    "    t = re.sub(r\"\\ [A-Za-z]*\\.com\", ' ', t)\n",
    "    t = re.sub(r\"@\\S+\", '', t)\n",
    "    t = t.replace('@', '')\n",
    "    t = unidecode.unidecode(t)\n",
    "    t = t.replace('#', '_')\n",
    "    \n",
    "    to_replace = [\"&quot;\", ':&lt;', ':&gt;', '&amp;', '-&lt;', '-&gt;', '=&lt;', '=&gt;', 's&lt;', 's&gt;']\n",
    "    for x in to_replace:\n",
    "        t = t.replace(x, '')\n",
    "    pattern = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    t = pattern.sub(r\"\\1\\1\", t)\n",
    "    \n",
    "    pattern = re.compile(r\"([\\s.,\\/#!$%^&*?;:{}=_`()+-])\\1{1,}\")\n",
    "    t = pattern.sub(r'\\1', t)\n",
    "    t = re.sub(' {2,}', '', t)\n",
    "    t = t.lower()\n",
    "    t = t.strip()\n",
    "    t = t.rstrip()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe6f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of twitts into tensors\n",
    "def collate_fn(batch, tokenizer):\n",
    "    enc = tokenizer([x for x in batch], padding=True, return_tensors='pt')['input_ids']\n",
    "    inpt = enc.transpose(0, 1)\n",
    "    return inpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db59ae4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (encoder): LstmEncoder(\n",
       "    (embs): Embedding(2000, 128)\n",
       "    (embs_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (embs_dp): Dropout(p=0.1, inplace=False)\n",
       "    (lstm): LSTM(128, 128)\n",
       "  )\n",
       "  (fc): ClassifierHead(\n",
       "    (dense1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (dp): Dropout(p=0.1, inplace=False)\n",
       "    (dense2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Classifier(cfg['model']['architecture'])\n",
    "model.load_state_dict(torch.load('./models/lstm_128.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fe836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the data.\n",
    "# in:\n",
    "#    data: list of twitts\n",
    "# out:\n",
    "#    res: a list of tuples (twitt, pred)\n",
    "\n",
    "def predict(data: List[str]):\n",
    "    data_clean = [clean(t) for t in data]\n",
    "    with torch.inference_mode():\n",
    "        preds = model.forward(collate_fn(data_clean, tokenizer))\n",
    "        preds = torch.sigmoid(preds.flatten())\n",
    "    return list(zip(data, preds.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d02700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"I just created my first LaTeX file from scratch. That didn't work out very well. (See @amandabittner , it's a great time waster)\",\n",
    "    \"AHH YES LOL IMA TELL MY HUBBY TO GO GET ME SUM MCDONALDS =]\",\n",
    "    \"RT @shrop: Awesome JQuery reference book for Coda! http://www.macpeeps.com/coda/ #webdesign\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f02f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I just created my first LaTeX file from scratch. That didn't work out very well. (See @amandabittner , it's a great time waster)\",\n",
       "  0.25448372960090637),\n",
       " ('AHH YES LOL IMA TELL MY HUBBY TO GO GET ME SUM MCDONALDS =]',\n",
       "  0.8318528532981873),\n",
       " ('RT @shrop: Awesome JQuery reference book for Coda! http://www.macpeeps.com/coda/ #webdesign',\n",
       "  0.9800198078155518)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e6878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
